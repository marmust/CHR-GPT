{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_char_encoding_size = 512\n",
    "input_length = 50\n",
    "\n",
    "with open(r\"C:\\Users\\user\\Desktop\\AUTOENCODER\\dataset_save.txt\", 'r') as f:\n",
    "        file_content = f.read()\n",
    "character_types = set(file_content)\n",
    "tokens = ''.join(character_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (em): Embedding(69, 512)\n",
       "  (fc1): Linear(in_features=25600, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc6): Linear(in_features=1024, out_features=68, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(len(tokens) + 1, per_char_encoding_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(per_char_encoding_size * input_length, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 1024)\n",
    "        self.fc5 = nn.Linear(1024, 1024)\n",
    "        self.fc6 = nn.Linear(1024, len(tokens))\n",
    "        \n",
    "        #nn.init.uniform_(self.fc1.weight, a=-1, b=1)\n",
    "        #nn.init.uniform_(self.fc2.weight, a=-1, b=1)\n",
    "        #nn.init.uniform_(self.fc3.weight, a=-1, b=1)\n",
    "        #nn.init.uniform_(self.fc4.weight, a=-1, b=1)\n",
    "        #nn.init.uniform_(self.fc5.weight, a=-1, b=1)\n",
    "        #nn.init.uniform_(self.fc6.weight, a=-1, b=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.em(x)\n",
    "        \n",
    "        if len(x.shape) == 3:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        else:\n",
    "            x = x.flatten()\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        x = torch.tanh(self.fc5(x))\n",
    "        x = torch.sigmoid(self.fc6(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "GPU = torch.device(\"cuda\")\n",
    "net.to(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(tensor, length):\n",
    "    current_length = tensor.shape[0]\n",
    "\n",
    "    # Pad or truncate the tensor to the desired length\n",
    "    if current_length < length:\n",
    "        padding = torch.zeros((length - current_length, *tensor.shape[1:]), dtype=tensor.dtype, device=tensor.device) + len(tokens)\n",
    "        padded_or_truncated_tensor = torch.cat((padding, tensor), dim=0)\n",
    "    elif current_length > length:\n",
    "        padded_or_truncated_tensor = tensor[-length:]\n",
    "    else:\n",
    "        padded_or_truncated_tensor = tensor\n",
    "\n",
    "    return padded_or_truncated_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_encode(text, tokens, length):\n",
    "    encoded = torch.zeros(len(text), dtype=torch.int32, device=GPU)\n",
    "    \n",
    "    for x in range(len(text)):\n",
    "        encoded[x] = tokens.find(text[x].lower())\n",
    "\n",
    "    return pad_or_truncate(encoded, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_decode(tensor, tokens):\n",
    "    return tokens[torch.argmax(tensor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chr(character, tokens):\n",
    "    encoded = torch.zeros(len(tokens), dtype=torch.float32, device=GPU)\n",
    "    encoded[tokens.find(character)] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(question_tensor, answer_tensor, model, loss=loss, optimizer=optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(question_tensor)\n",
    "    loss = loss(outputs, answer_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsDataset(Dataset):\n",
    "    def __init__(self, file_path, tokens, input_length):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            word_list = f.read().split(\"\\n\")\n",
    "        \n",
    "        self.cut_off_words = []\n",
    "        self.completions = []\n",
    "        \n",
    "        for x in tqdm(word_list):\n",
    "            for y in range(len(x) - 1):\n",
    "                self.cut_off_words.append(tokenizer_encode(x[:y + 1], tokens, input_length))\n",
    "                self.completions.append(encode_chr(x[y + 1], tokens))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.completions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cut_off_words[idx], self.completions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_save_txt_pickle.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=10640, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 116/500 [05:22<16:37,  2.60s/it]"
     ]
    }
   ],
   "source": [
    "# i know its overfitted, its a proof of concept, you can train for less\n",
    "for y in tqdm(range(500)):\n",
    "        for x in data_loader:\n",
    "                train_nn(x[0], x[1], net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:07<00:00, 71.30it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'espires areessersess atio sisethess soand the plactime and meapention, gone fores are ations led to for fiminat ward int mountess and underss ay ueathe ses and to ard and porsenss ficeltorg anmding and ind coftion to nuspecseng ay te tetoons. ndesstinition impoctlve to ase prooticat besining ations to intly an they oritive cand take breake s to ithe ritore to eustione fondertested and oander. the wirll. ingyour shale pcons are and caan to yations to pono nond to treatitins to tan hite and novels'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = \"\"\n",
    "completion = start\n",
    "\n",
    "for x in tqdm(range(500)):\n",
    "    completion += tokenizer_decode(net(tokenizer_encode(completion, tokens, input_length)), tokens)\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:39<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = WordsDataset(r\"C:\\Users\\user\\Desktop\\AUTOENCODER\\dataset_save.txt\", tokens, input_length)\n",
    "data_loader = DataLoader(dataset, batch_size=550, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_save_txt_pickle.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), r\"model_for_writing_somewhat_words.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIN_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
